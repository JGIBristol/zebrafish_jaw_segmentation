{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fcd37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "features_df = pd.read_csv(\"features.csv\", index_col=0)\n",
    "\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c6fea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add a column describing the mutation status (wt/het/hom/mosaic)\n",
    "\"\"\"\n",
    "\n",
    "from fishjaw.inference import feature_selection\n",
    "\n",
    "features_df = feature_selection.add_metadata_cols(features_df)\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d00f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Remove features with zero variance\n",
    "\"\"\"\n",
    "\n",
    "null_variance_cols = features_df[\"Features\"].columns[features_df[\"Features\"].var() == 0]\n",
    "features_df.drop(columns=null_variance_cols, inplace=True, level=1)\n",
    "\n",
    "print(f\"Dropped:\\n\\t\", \", \".join(null_variance_cols))\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc727d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot correlations\n",
    "\"\"\"\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "corr = features_df[\"Features\"].corr()\n",
    "sns.heatmap(corr, vmin=-1, vmax=1, cmap=\"seismic\")\n",
    "\n",
    "c = np.abs(corr.to_numpy().flat)\n",
    "c[c == 1.0] = np.nan\n",
    "\n",
    "fig, axis = plt.subplots()\n",
    "axis.hist(c, bins=100)\n",
    "axis.set_title(r\"$\\left|\\mathrm{Correlations}\\right|$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994c95d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Drop highly correlated features\n",
    "\"\"\"\n",
    "\n",
    "from typing import Iterable, Tuple, List, Optional\n",
    "\n",
    "\n",
    "def drop_correlated_features(\n",
    "    df: pd.DataFrame,\n",
    "    threshold: float = 0.8,\n",
    "    protected: Optional[Iterable[str]] = None,\n",
    "    prefer: str = \"lower_variance\",  # or \"higher_variance\" or \"mean_corr\"\n",
    ") -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Greedily drop a minimal-ish set of columns so that all remaining\n",
    "    pairwise absolute correlations are <= threshold.\n",
    "\n",
    "    - protected: columns never to drop (will raise if impossible).\n",
    "    - prefer: tie-breaker when choosing what to drop among highly connected nodes.\n",
    "    \"\"\"\n",
    "    if not 0 <= threshold <= 1:\n",
    "        raise ValueError(\"threshold must be in [0, 1]\")\n",
    "\n",
    "    prot = set(protected or [])\n",
    "\n",
    "    # Absolute correlation matrix\n",
    "    corr = df.corr().abs()\n",
    "    # Remove self-correlation to simplify logic\n",
    "    np.fill_diagonal(corr.values, 0.0)\n",
    "    # Replace NaNs with 0 (e.g., constant columns). Ideally drop NaNs beforehand.\n",
    "    corr = corr.fillna(0.0)\n",
    "\n",
    "    to_drop: List[str] = []\n",
    "    remaining = corr.index.tolist()\n",
    "\n",
    "    while True:\n",
    "        # Edges above threshold\n",
    "        mask = corr > threshold\n",
    "        if not mask.values.any():\n",
    "            break\n",
    "\n",
    "        # Degree = number of correlations above threshold\n",
    "        deg = mask.sum(axis=1)\n",
    "\n",
    "        # Candidate nodes with max degree\n",
    "        max_deg = deg.max()\n",
    "        cand = deg[deg == max_deg].index.tolist()\n",
    "\n",
    "        # Apply tie-breaker\n",
    "        if prefer == \"lower_variance\":\n",
    "            var = df[cand].var(numeric_only=True)\n",
    "            pick = var.idxmin()\n",
    "        elif prefer == \"higher_variance\":\n",
    "            var = df[cand].var(numeric_only=True)\n",
    "            pick = var.idxmax()\n",
    "        elif prefer == \"mean_corr\":\n",
    "            mc = corr.loc[cand].mean(axis=1)\n",
    "            pick = mc.idxmax()\n",
    "        else:\n",
    "            pick = cand[0]  # deterministic order if possible\n",
    "\n",
    "        if pick in prot:\n",
    "            # If protected is involved in edges, try dropping the most offending non-protected neighbor\n",
    "            # Choose neighbor with largest correlation to the protected node\n",
    "            neighbors = corr.columns[mask.loc[pick]]\n",
    "            neighbors = [n for n in neighbors if n not in prot]\n",
    "            if not neighbors:\n",
    "                raise RuntimeError(\n",
    "                    f\"Cannot satisfy threshold={threshold} without dropping protected feature '{pick}'\"\n",
    "                )\n",
    "            # Choose neighbor with highest correlation to the protected pick\n",
    "            pick = corr.loc[pick, neighbors].idxmax()\n",
    "\n",
    "        # Drop the picked column/row from the working correlation matrix\n",
    "        to_drop.append(pick)\n",
    "        corr = corr.drop(index=pick, columns=pick)\n",
    "        remaining.remove(pick)\n",
    "\n",
    "    return remaining, to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787cd373",
   "metadata": {},
   "outputs": [],
   "source": [
    "kept, dropped = drop_correlated_features(features_df[\"Features\"], threshold=0.8)\n",
    "# Keep only 'kept'\n",
    "features_df.drop(columns=dropped, level=1, inplace=True)\n",
    "print(f\"Dropped {len(dropped)} cols:\\n\\t\", \", \".join(dropped))\n",
    "\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed2888",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = features_df[\"Features\"].corr()\n",
    "sns.heatmap(corr, vmin=-1, vmax=1, cmap=\"seismic\")\n",
    "\n",
    "c = np.abs(corr.to_numpy().flat)\n",
    "c[c == 1.0] = np.nan\n",
    "\n",
    "fig, axis = plt.subplots()\n",
    "axis.hist(c, bins=100)\n",
    "axis.set_title(r\"$\\left|\\mathrm{Correlations}\\right|$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333a6cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Z-normalise features\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def normalise(df):\n",
    "    X = df[\"Features\"]\n",
    "    mu = X.mean()\n",
    "    sigma = X.std(ddof=0)\n",
    "\n",
    "    df[\"Features\"] = (X - mu) / sigma.replace(0.0, np.nan)\n",
    "    return df\n",
    "\n",
    "\n",
    "features_df = normalise(features_df)\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee16a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Mann-Whitney U\"\"\"\n",
    "\n",
    "from scipy.stats import mannwhitneyu\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "target_col = (\"Metadata\", \"genotype\")\n",
    "\n",
    "def feature_tests(df, target_col, target_val):\n",
    "    \"\"\"\n",
    "    Mann-Whitney U test for each feature between WT and mutant fish.\n",
    "    \"\"\"\n",
    "    X = df[\"Features\"]\n",
    "    y = df[target_col]\n",
    "\n",
    "    results = []\n",
    "    for col in X.columns:\n",
    "        group1 = X.loc[y == target_val, col]  # WT\n",
    "        group2 = X.loc[y != target_val, col]  # mutant\n",
    "        stat, p = mannwhitneyu(group1, group2, alternative=\"two-sided\")\n",
    "        auc = np.mean(\n",
    "            [val > group2.median() for val in group1]\n",
    "        )  # quick effect size proxy\n",
    "        results.append((col, stat, p, auc))\n",
    "    df = pd.DataFrame(results, columns=[\"feature\", \"U\", \"pval\", \"effect_size\"])\n",
    "    df[\"pval_adj\"] = multipletests(df.pval, method=\"fdr_bh\")[1]\n",
    "    return df.sort_values(\"pval_adj\")\n",
    "\n",
    "\n",
    "mwu_results = feature_tests(features_df, target_col, \"wt\")\n",
    "mwu_results.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb9075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def rf_tests(df, target_col):\n",
    "    \"\"\"\n",
    "    Random forest feature importance.\n",
    "    \"\"\"\n",
    "    X = df[\"Features\"]\n",
    "    y = df[target_col]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    rf = GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=0,\n",
    "        max_depth=2,\n",
    "        learning_rate=0.1,\n",
    "        min_samples_leaf=10,\n",
    "        min_samples_split=10,\n",
    "        subsample=0.9,\n",
    "        criterion=\"friedman_mse\",\n",
    "        max_features=3,\n",
    "    )\n",
    "\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Check the score - should be pretty good\n",
    "    print(\"Train score: \", rf.score(X_train, y_train))\n",
    "    print(\"Test score: \", rf.score(X_test, y_test))\n",
    "\n",
    "    importances = pd.Series(rf.feature_importances_, index=X_train.columns)\n",
    "\n",
    "    # Conver to df to make notebook output nicer\n",
    "    return pd.DataFrame(importances.sort_values(ascending=False))\n",
    "\n",
    "\n",
    "rf_results = rf_tests(features_df, target_col)\n",
    "rf_results.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5ea668",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot 1d distributions of top features\n",
    "\"\"\"\n",
    "\n",
    "import textwrap\n",
    "\n",
    "\n",
    "def plot_feature_dists(df, mwu_results, rf_results, target_col):\n",
    "    features = list(\n",
    "        set(\n",
    "            (\n",
    "                mwu_results.head(5)[\"feature\"].tolist()\n",
    "                + rf_results.head(5).index.tolist()\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    plot_df = df[[(\"Features\", f) for f in features] + [target_col]]\n",
    "\n",
    "    pairgrid = sns.pairplot(\n",
    "        plot_df,\n",
    "        hue=target_col,\n",
    "        plot_kws={\"alpha\": 0.5, \"s\": 5},\n",
    "        diag_kind=\"kde\",\n",
    "        diag_kws=dict(common_norm=False),\n",
    "    )\n",
    "\n",
    "    # Set x and y labels to the feature names only\n",
    "    for ax in pairgrid.axes.flatten():\n",
    "        if ax is not None:\n",
    "            if xlabel := ax.get_xlabel():\n",
    "                ax.set_xlabel(textwrap.fill(xlabel.split(\",\")[1].strip(), 20))\n",
    "\n",
    "            if ylabel := ax.get_ylabel():\n",
    "                ax.set_ylabel(textwrap.fill(ax.get_ylabel().split(\",\")[1].strip(), 20))\n",
    "\n",
    "\n",
    "plot_feature_dists(features_df, mwu_results, rf_results, target_col=target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fa358b",
   "metadata": {},
   "source": [
    "Let's repeat these, but selecting by age...\n",
    "====\n",
    "We will want to only take wildtypes for this - I'll take those with a genotype of \"wt\" or \"het\" (these will probably be wild-enough-type...)\n",
    "\n",
    "We'll select young and older fish as our age groups - see below for the groupings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ced152",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Keep only wildtype-ish\n",
    "\"\"\"\n",
    "age_df = features_df[features_df[(\"Metadata\", \"genotype\")].isin([\"wt\", \"het\"])].copy()\n",
    "age_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3713766",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split by age\n",
    "\"\"\"\n",
    "\n",
    "def age_grouper(age):\n",
    "    age = int(age)\n",
    "    if age == -1:\n",
    "        return np.nan\n",
    "\n",
    "    if age <= 12:\n",
    "        return 0\n",
    "    if age >= 30:\n",
    "        return 1\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "age_df.loc[:, (\"Metadata\", \"age_group\")] = age_df[(\"Metadata\", \"age\")].map(\n",
    "    age_grouper\n",
    ")\n",
    "\n",
    "age_df = age_df.dropna(subset=[(\"Metadata\", \"age_group\")])\n",
    "age_df[(\"Metadata\", \"age_group\")].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f638353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Repeat the above feature selection steps, but for age groups\n",
    "\"\"\"\n",
    "\n",
    "target_col = (\"Metadata\", \"age_group\")\n",
    "age_df = normalise(age_df)\n",
    "\n",
    "mwu_results = feature_tests(age_df, target_col, 0)\n",
    "rf_results = rf_tests(age_df, target_col)\n",
    "\n",
    "display(mwu_results)\n",
    "display(rf_results)\n",
    "\n",
    "plot_feature_dists(age_df, mwu_results, rf_results, target_col)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fishjaw (3.11.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
