{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb27ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f5d6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fishjaw.util import util\n",
    "\n",
    "config = util.userconf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f33319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Read in a small number of CT scans and the corresponding labels\"\"\"\n",
    "\n",
    "import pathlib\n",
    "\n",
    "\n",
    "dicom_dir = pathlib.Path(\"../../dicoms/Training set 2/\")\n",
    "dicom_paths = sorted(list(dicom_dir.glob(\"*.dcm\")))\n",
    "print(len(dicom_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08766533",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths, val_paths, test_paths = (\n",
    "    dicom_paths[:15],\n",
    "    dicom_paths[15:18],\n",
    "    dicom_paths[18:],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d251bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from fishjaw.images import io\n",
    "\n",
    "train_imgs, train_labels = zip(*[io.read_dicom(path) for path in tqdm(train_paths)])\n",
    "test_imgs, test_labels = zip(*[io.read_dicom(path) for path in tqdm(test_paths)])\n",
    "val_imgs, val_labels = zip(*[io.read_dicom(path) for path in tqdm(val_paths)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b7d16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import center_of_mass\n",
    "\n",
    "train_centroids = [\n",
    "    tuple(map(int, center_of_mass(label))) for label in tqdm(train_labels)\n",
    "]\n",
    "val_centroids = [tuple(map(int, center_of_mass(label))) for label in tqdm(val_labels)]\n",
    "test_centroids = [tuple(map(int, center_of_mass(label))) for label in tqdm(test_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a7fa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot the centroids\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axis = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "img_slice = train_imgs[0][train_centroids[0][0]]\n",
    "label_slice = train_labels[0][train_centroids[0][0]]\n",
    "\n",
    "axis.imshow(img_slice, cmap=\"gray\")\n",
    "axis.imshow(label_slice, cmap=\"afmhot_r\", alpha=0.3)\n",
    "\n",
    "axis.scatter(\n",
    "    train_centroids[0][2], train_centroids[0][1], color=\"red\", s=20, label=\"Centroid\"\n",
    ")\n",
    "axis.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcc8110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we don't really need all the information of the high-resolution images,\n",
    "# we can downsample them to speed up processing.\n",
    "import numpy as np\n",
    "\n",
    "# ndimage.zoom is faster but i think skimage.resize is more accurate\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "output_size = (512, 128, 128)\n",
    "\n",
    "\n",
    "def resize_image_and_label(imgs, labels, centroids, target_shape):\n",
    "    scale_factors = []\n",
    "    resized_imgs = []\n",
    "    resized_labels = []\n",
    "    scaled_centroids = []\n",
    "\n",
    "    for img, label, centroid in tqdm(\n",
    "        zip(imgs, labels, centroids, strict=True), total=len(imgs)\n",
    "    ):\n",
    "        assert img.shape == label.shape, \"Image and label must have the same shape\"\n",
    "\n",
    "        # Calculate scale factors for each dimension\n",
    "        scale_factor = tuple(\n",
    "            target_dim / orig_dim\n",
    "            for orig_dim, target_dim in zip(img.shape, output_size)\n",
    "        )\n",
    "        scale_factors.append(scale_factor)\n",
    "\n",
    "        # Resize image and label using zoom\n",
    "        resized_img = zoom(\n",
    "            img, scale_factor, order=3\n",
    "        )  # Use cubic interpolation for images\n",
    "        resized_label = zoom(\n",
    "            label, scale_factor, order=0\n",
    "        )  # Use nearest-neighbor for labels\n",
    "\n",
    "        resized_imgs.append(resized_img)\n",
    "        resized_labels.append(resized_label)\n",
    "\n",
    "        # Rescale centroid to match the resized image\n",
    "        scaled_centroid = tuple(int(c * sf) for c, sf in zip(centroid, scale_factor))\n",
    "        scaled_centroids.append(scaled_centroid)\n",
    "\n",
    "    print(np.mean(scale_factors, axis=0), np.std(scale_factors, axis=0))\n",
    "\n",
    "    return resized_imgs, resized_labels, scaled_centroids, scale_factors\n",
    "\n",
    "\n",
    "train_imgs, train_labels, train_centroids, _ = resize_image_and_label(\n",
    "    train_imgs, train_labels, train_centroids, output_size\n",
    ")\n",
    "val_imgs, val_labels, val_centroids, _ = resize_image_and_label(\n",
    "    val_imgs, val_labels, val_centroids, output_size\n",
    ")\n",
    "\n",
    "# Keep the test set at original resolution for evaluation\n",
    "resized_test_imgs, resized_test_labels, resized_test_centroids, test_scale_factors = (\n",
    "    resize_image_and_label(test_imgs, test_labels, test_centroids, output_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efd953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "img_slice = resized_test_imgs[0][resized_test_centroids[0][0]]\n",
    "label_slice = resized_test_labels[0][resized_test_centroids[0][0]]\n",
    "\n",
    "axis.imshow(img_slice, cmap=\"gray\")\n",
    "axis.imshow(label_slice, cmap=\"afmhot_r\", alpha=0.3)\n",
    "\n",
    "axis.scatter(\n",
    "    resized_test_centroids[0][2],\n",
    "    resized_test_centroids[0][1],\n",
    "    color=\"red\",\n",
    "    s=20,\n",
    "    label=\"Centroid\",\n",
    ")\n",
    "axis.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a223c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model arch\n",
    "\n",
    "from monai.networks.nets import AttentionUnet\n",
    "\n",
    "model = AttentionUnet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    strides=(2, 2, 2),\n",
    "    channels=(4, 8, 16, 32),\n",
    "    dropout=0.05,\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8924cae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "\n",
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, images, centroids, sigma):\n",
    "        self.data = torch.tensor(\n",
    "            np.array(images, dtype=np.float32), dtype=torch.float32\n",
    "        ).unsqueeze(1)\n",
    "\n",
    "        self.centroids = []\n",
    "        for img, centroid in zip(images, centroids):\n",
    "            gaussian_mask = np.zeros_like(img, dtype=np.float32)\n",
    "            gaussian_mask[centroid[0], centroid[1], centroid[2]] = 1  # Set the centroid\n",
    "            gaussian_mask = gaussian_filter(\n",
    "                gaussian_mask, sigma=sigma\n",
    "            )  # Apply Gaussian filter\n",
    "\n",
    "            # Normalise to sum to 1 to make it a valid probability distribution\n",
    "            gaussian_mask /= np.sum(gaussian_mask) if np.sum(gaussian_mask) > 0 else 1\n",
    "\n",
    "            self.centroids.append(gaussian_mask)\n",
    "\n",
    "        self.centroids = torch.tensor(\n",
    "            np.array(self.centroids), dtype=torch.float32\n",
    "        ).unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx].to(\"cuda\"), self.centroids[idx].to(\"cuda\")\n",
    "\n",
    "\n",
    "# Initialize\n",
    "sigma = 2\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "train_dataset = ImgDataset(train_imgs, train_centroids, sigma=sigma)\n",
    "val_dataset = ImgDataset(val_imgs, val_centroids, sigma=sigma)\n",
    "\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b2411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first thing in the dataloader\n",
    "\n",
    "\n",
    "def plot_sample(img, centroid):\n",
    "    # Find the index of the Z slice where the centroid has the highest sum\n",
    "    centre = torch.argmax(centroid[0][0].sum(dim=(1, 2))).item()\n",
    "\n",
    "    fig, axis = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    img_slice = img[0][0][centre].cpu().numpy()\n",
    "    centroid_slice = centroid[0][0][centre].cpu().numpy()\n",
    "    axis.imshow(img_slice, cmap=\"gray\")\n",
    "    axis.imshow(centroid_slice, cmap=\"afmhot_r\", alpha=0.3)\n",
    "\n",
    "    axis.axis(\"off\")\n",
    "\n",
    "\n",
    "plot_sample(*next(iter(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a65748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def kl_heatmap_loss(pred, target):\n",
    "    # Apply log-softmax to predictions\n",
    "    pred = F.log_softmax(pred.view(pred.size(0), -1), dim=1)\n",
    "\n",
    "    # Ensure target is normalized (if not already)\n",
    "    target = target.view(pred.size(0), -1)\n",
    "    target = target / target.sum(dim=1, keepdim=True)\n",
    "\n",
    "    # Compute KL divergence\n",
    "    return F.kl_div(pred, target, reduction=\"batchmean\")\n",
    "\n",
    "\n",
    "def train(model, train_data, val_data, n_epochs):\n",
    "    model.train()\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    pbar = tqdm(range(n_epochs), total=n_epochs, desc=\"Training Epochs\")\n",
    "    for epoch in pbar:\n",
    "        train_loss, val_loss = [], []\n",
    "\n",
    "        for i, (volumes, coords) in enumerate(train_data):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(volumes)\n",
    "            loss = kl_heatmap_loss(outputs, coords)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        for i, (volumes, coords) in enumerate(val_data):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(volumes)\n",
    "                loss = kl_heatmap_loss(outputs, coords)\n",
    "                val_loss.append(loss.item())\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        pbar.set_postfix(train_loss=np.mean(train_loss), val_loss=np.mean(val_loss))\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "n_epochs = 100\n",
    "train_losses, val_losses = train(model, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10ae18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fishjaw.visualisation import training\n",
    "\n",
    "fig = training.plot_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f5451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model - visualise the heatmap\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_vol = (\n",
    "        torch.tensor(\n",
    "            np.array(resized_test_imgs[0], dtype=np.float32), dtype=torch.float32\n",
    "        )\n",
    "        .unsqueeze(0)\n",
    "        .unsqueeze(0)\n",
    "        .to(\"cuda\")\n",
    "    )\n",
    "\n",
    "    predicted_heatmap = model(test_vol)\n",
    "\n",
    "    plot_sample(test_vol, predicted_heatmap)\n",
    "\n",
    "    # Repeat but rotate to have different axis orientations\n",
    "    plot_sample(\n",
    "        test_vol.permute(0, 1, 3, 2, 4), predicted_heatmap.permute(0, 1, 3, 2, 4)\n",
    "    )\n",
    "    plot_sample(\n",
    "        test_vol.permute(0, 1, 4, 2, 3), predicted_heatmap.permute(0, 1, 4, 2, 3)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4476ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted co-ords from the maximum along each axis\n",
    "def find_gaussian_center(heatmap):\n",
    "    \"\"\"\n",
    "    Find the center of a 3D Gaussian heatmap using a convolution-based approach.\n",
    "\n",
    "    Args:\n",
    "        heatmap (torch.Tensor): A 5D tensor of shape (batch, channel, depth, height, width).\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[int, int, int]]: The indices of the center (z, y, x) for each heatmap in the batch.\n",
    "    \"\"\"\n",
    "    # Define a 3D Gaussian kernel (or a box kernel)\n",
    "    kernel_size = 5  # Size of the kernel\n",
    "    sigma = 1.0  # Standard deviation for the Gaussian kernel\n",
    "    coords = torch.arange(kernel_size, device=heatmap.device) - (kernel_size - 1) / 2\n",
    "    z, y, x = torch.meshgrid(coords, coords, coords, indexing=\"ij\")\n",
    "    gaussian_kernel = torch.exp(-(z**2 + y**2 + x**2) / (2 * sigma**2))\n",
    "    gaussian_kernel = gaussian_kernel / gaussian_kernel.sum()  # Normalize the kernel\n",
    "    gaussian_kernel = gaussian_kernel.view(\n",
    "        1, 1, *gaussian_kernel.shape\n",
    "    )  # Add batch and channel dims\n",
    "\n",
    "    # Apply the convolution\n",
    "    smoothed_heatmap = F.conv3d(heatmap, gaussian_kernel, padding=kernel_size // 2)\n",
    "\n",
    "    # Find the voxel with the maximum value\n",
    "    flat_idx = torch.argmax(smoothed_heatmap.view(smoothed_heatmap.size(0), -1), dim=1)\n",
    "    batch_size, _, depth, height, width = heatmap.shape\n",
    "    z = flat_idx // (height * width)\n",
    "    y = (flat_idx % (height * width)) // width\n",
    "    x = flat_idx % width\n",
    "\n",
    "    # Combine the coordinates into a list of tuples\n",
    "    centers = [(z[i].item(), y[i].item(), x[i].item()) for i in range(batch_size)]\n",
    "    return centers\n",
    "\n",
    "\n",
    "(predicted_coords,) = find_gaussian_center(predicted_heatmap)\n",
    "predicted_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f12cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot slices of the cropped jaw\n",
    "fig, axis = plt.subplots(1, 1)\n",
    "\n",
    "img_slice = resized_test_imgs[0][int(predicted_coords[0])]\n",
    "label_slice = resized_test_labels[0][int(predicted_coords[0])]\n",
    "\n",
    "axis.imshow(img_slice, cmap=\"gray\")\n",
    "axis.imshow(label_slice, cmap=\"afmhot_r\", alpha=0.3)\n",
    "axis.scatter(\n",
    "    predicted_coords[2],\n",
    "    predicted_coords[1],\n",
    "    color=\"red\",\n",
    "    s=20,\n",
    "    label=\"Predicted Centroid\",\n",
    ")\n",
    "axis.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f8e624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the prediction back up\n",
    "scaled_prediction = tuple(\n",
    "    int(coord / scale_factor)\n",
    "    for coord, scale_factor in zip(predicted_coords, test_scale_factors[0])\n",
    ")\n",
    "print(f\"Scaled prediction: {scaled_prediction}\")\n",
    "\n",
    "fig, axis = plt.subplots(1, 1)\n",
    "\n",
    "img_slice = test_imgs[0][int(scaled_prediction[0])]\n",
    "label_slice = test_labels[0][int(scaled_prediction[0])]\n",
    "\n",
    "axis.imshow(img_slice, cmap=\"gray\")\n",
    "axis.imshow(label_slice, cmap=\"afmhot_r\", alpha=0.3)\n",
    "axis.scatter(\n",
    "    scaled_prediction[2],\n",
    "    scaled_prediction[1],\n",
    "    color=\"red\",\n",
    "    s=20,\n",
    "    label=\"Predicted Centroid\",\n",
    ")\n",
    "axis.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zebrafish_jaw_segmentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
