{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb27ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f5d6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fishjaw.util import util\n",
    "\n",
    "config = util.userconf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f33319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Read in a small number of CT scans and the corresponding labels\"\"\"\n",
    "\n",
    "import pathlib\n",
    "\n",
    "\n",
    "dicom_dir = pathlib.Path(\"../../dicoms/Training set 2/\")\n",
    "dicom_paths = sorted(list(dicom_dir.glob(\"*.dcm\")))\n",
    "print(len(dicom_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08766533",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_paths, val_paths, test_paths = (\n",
    "    dicom_paths[:15],\n",
    "    dicom_paths[15:18],\n",
    "    dicom_paths[18:],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d251bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from fishjaw.images import io\n",
    "\n",
    "train_imgs, train_labels = zip(*[io.read_dicom(path) for path in tqdm(train_paths)])\n",
    "test_imgs, test_labels = zip(*[io.read_dicom(path) for path in tqdm(test_paths)])\n",
    "val_imgs, val_labels = zip(*[io.read_dicom(path) for path in tqdm(val_paths)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b7d16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import center_of_mass\n",
    "\n",
    "train_centroids = [\n",
    "    tuple(map(int, center_of_mass(label))) for label in tqdm(train_labels)\n",
    "]\n",
    "val_centroids = [tuple(map(int, center_of_mass(label))) for label in tqdm(val_labels)]\n",
    "test_centroids = [tuple(map(int, center_of_mass(label))) for label in tqdm(test_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a7fa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot the centroids\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "img_slice = img[centroid[0]]\n",
    "label_slice = label[centroid[0]]\n",
    "\n",
    "axis.imshow(img_slice, cmap=\"gray\")\n",
    "axis.imshow(label_slice, cmap=\"afmhot_r\", alpha=0.3)\n",
    "\n",
    "axis.scatter(centroid[2], centroid[1], color=\"red\", s=20, label=\"Centroid\")\n",
    "axis.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcc8110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we don't really need all the information of the high-resolution images,\n",
    "# we can downsample them to speed up processing.\n",
    "\n",
    "# ndimage.zoom is faster but i think skimage.resize is more accurate\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "output_size = (128, 128, 128)\n",
    "\n",
    "\n",
    "def resize_image_and_label(imgs, labels, centroids, target_shape):\n",
    "    scale_factors = []\n",
    "    resized_imgs = []\n",
    "    resized_labels = []\n",
    "    scaled_centroids = []\n",
    "\n",
    "    for img, label, centroid in tqdm(\n",
    "        zip(imgs, labels, centroids, strict=True), total=len(imgs)\n",
    "    ):\n",
    "        assert img.shape == label.shape, \"Image and label must have the same shape\"\n",
    "\n",
    "        # Calculate scale factors for each dimension\n",
    "        scale_factor = tuple(\n",
    "            target_dim / orig_dim\n",
    "            for orig_dim, target_dim in zip(img.shape, output_size)\n",
    "        )\n",
    "        scale_factors.append(scale_factor)\n",
    "\n",
    "        # Resize image and label using zoom\n",
    "        resized_img = zoom(\n",
    "            img, scale_factor, order=3\n",
    "        )  # Use cubic interpolation for images\n",
    "        resized_label = zoom(\n",
    "            label, scale_factor, order=0\n",
    "        )  # Use nearest-neighbor for labels\n",
    "\n",
    "        resized_imgs.append(resized_img)\n",
    "        resized_labels.append(resized_label)\n",
    "\n",
    "        # Rescale centroid to match the resized image\n",
    "        scaled_centroid = tuple(int(c * sf) for c, sf in zip(centroid, scale_factor))\n",
    "        scaled_centroids.append(scaled_centroid)\n",
    "\n",
    "    return resized_imgs, resized_labels, scaled_centroids, scale_factors\n",
    "\n",
    "\n",
    "train_imgs, train_labels, train_centroids, _ = resize_image_and_label(\n",
    "    train_imgs, train_labels, train_centroids, output_size\n",
    ")\n",
    "val_imgs, val_labels, val_centroids, _ = resize_image_and_label(\n",
    "    val_imgs, val_labels, val_centroids, output_size\n",
    ")\n",
    "\n",
    "# Keep the test set at original resolution for evaluation\n",
    "resized_test_imgs, resized_test_labels, resized_test_centroids, test_scale_factors = (\n",
    "    resize_image_and_label(test_imgs, test_labels, test_centroids, output_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efd953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "img_slice = resized_test_imgs[0][resized_test_centroids[0][0]]\n",
    "label_slice = resized_test_imgs[0][resized_test_centroids[0][0]]\n",
    "\n",
    "axis.imshow(img_slice, cmap=\"gray\")\n",
    "axis.imshow(label_slice, cmap=\"afmhot_r\", alpha=0.3)\n",
    "\n",
    "axis.scatter(\n",
    "    resized_test_centroid[0][2],\n",
    "    resized_test_centroid[0][1],\n",
    "    color=\"red\",\n",
    "    s=20,\n",
    "    label=\"Centroid\",\n",
    ")\n",
    "axis.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a223c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model arch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Simple3DCNNRegressor(nn.Module):\n",
    "    def __init__(self, in_channels=1):\n",
    "        super(Simple3DCNNRegressor, self).__init__()\n",
    "\n",
    "        def conv_block(in_c, out_c):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv3d(in_c, out_c, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm3d(out_c),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv3d(out_c, out_c, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm3d(out_c),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool3d(2),\n",
    "            )\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            conv_block(in_channels, 32),  # -> [B, 32, D/2, H/2, W/2]\n",
    "            conv_block(32, 64),  # -> [B, 64, D/4, H/4, W/4]\n",
    "            conv_block(64, 128),  # -> [B, 128, D/8, H/8, W/8]\n",
    "            conv_block(128, 256),  # -> [B, 256, D/16, H/16, W/16]\n",
    "            nn.AdaptiveAvgPool3d(1),  # -> [B, 256, 1, 1, 1]\n",
    "        )\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Flatten(),  # [B, 256]\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 3),  # Output: [X, Y, Z]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.regressor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8924cae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, images, centroids):\n",
    "        self.data = (\n",
    "            torch.tensor(np.array(images, dtype=np.float32), dtype=torch.float32)\n",
    "            .unsqueeze(1)\n",
    "            .to(\"cuda\")\n",
    "        )\n",
    "        self.centroids = torch.tensor(np.array(centroids), dtype=torch.float32).to(\n",
    "            \"cuda\"\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.centroids[idx]\n",
    "\n",
    "\n",
    "# Initialize\n",
    "model = Simple3DCNNRegressor().to(\"cuda\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_dataset = ImgDataset(train_imgs, train_centroids)\n",
    "val_dataset = ImgDataset(val_imgs, val_centroids)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a65748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  13%|█▎        | 13/100 [00:11<01:20,  1.08it/s, train_loss=6.33e+3, val_loss=5.73e+3]"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "def train(model, train_data, val_data, n_epochs):\n",
    "    model.train()\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    pbar = tqdm(range(n_epochs), total=n_epochs, desc=\"Training Epochs\")\n",
    "    for epoch in pbar:\n",
    "        train_loss, val_loss = [], []\n",
    "\n",
    "        for i, (volumes, coords) in enumerate(train_data):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(volumes)\n",
    "            loss = criterion(outputs, coords)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        for i, (volumes, coords) in enumerate(val_data):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(volumes)\n",
    "                loss = criterion(outputs, coords)\n",
    "                val_loss.append(loss.item())\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        pbar.set_postfix(train_loss=np.mean(train_loss), val_loss=np.mean(val_loss))\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "train_losses, val_losses = train(model, train_loader, val_loader, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10ae18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fishjaw.visualisation import training\n",
    "\n",
    "fig = training.plot_losses(train_losses, val_losses)\n",
    "plt.ylim(0, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f5451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model - perform some cropping\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_vol = (\n",
    "        torch.tensor(\n",
    "            np.array(resized_test_imgs[0], dtype=np.float32), dtype=torch.float32\n",
    "        )\n",
    "        .unsqueeze(0)\n",
    "        .unsqueeze(0)\n",
    "        .to(\"cuda\")\n",
    "    )\n",
    "\n",
    "    predicted_coords = model(test_vol).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "print(f\"train data: Predicted {predicted_coords}, True: {resized_test_centroids[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f12cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot slices of the cropped jaw\n",
    "fig, axis = plt.subplots(1, 1)\n",
    "\n",
    "img_slice = resized_test_imgs[0][int(predicted_coords[0])]\n",
    "label_slice = resized_test_labels[0][int(predicted_coords[0])]\n",
    "\n",
    "axis.imshow(img_slice, cmap=\"gray\")\n",
    "axis.imshow(label_slice, cmap=\"afmhot_r\", alpha=0.3)\n",
    "axis.scatter(\n",
    "    predicted_coords[2],\n",
    "    predicted_coords[1],\n",
    "    color=\"red\",\n",
    "    s=20,\n",
    "    label=\"Predicted Centroid\",\n",
    ")\n",
    "axis.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f8e624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the prediction back up\n",
    "scaled_prediction = tuple(\n",
    "    int(coord / scale_factor)\n",
    "    for coord, scale_factor in zip(predicted_coords, test_scale_factors[0])\n",
    ")\n",
    "print(f\"Scaled prediction: {scaled_prediction}\")\n",
    "\n",
    "fig, axis = plt.subplots(1, 1)\n",
    "\n",
    "img_slice = test_imgs[0][int(scaled_prediction[0])]\n",
    "label_slice = test_labels[0][int(scaled_prediction[0])]\n",
    "\n",
    "axis.imshow(img_slice, cmap=\"gray\")\n",
    "axis.imshow(label_slice, cmap=\"afmhot_r\", alpha=0.3)\n",
    "axis.scatter(\n",
    "    scaled_prediction[2],\n",
    "    scaled_prediction[1],\n",
    "    color=\"red\",\n",
    "    s=20,\n",
    "    label=\"Predicted Centroid\",\n",
    ")\n",
    "axis.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zebrafish_jaw_segmentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
