{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfc5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add the zebrafish jaw lib to python path\n",
    "\"\"\"\n",
    "\n",
    "import os, sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\"\"), \"..\", \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365b25cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_plots = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30bdda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The first thing we'll do is read a model from disk - we want to access its config file\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from fishjaw.model import model\n",
    "\n",
    "model_name = \"attempt_3.pkl\"\n",
    "jaw_model = model.load_model(\"attempt_3.pkl\")\n",
    "jaw_config = jaw_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426f0b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We'll fine tune on some quadrates that Wahab segmented - we'll want to read these from the RDSF and crop to the right region of interest\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import pathlib\n",
    "\n",
    "import tifffile\n",
    "from tqdm import tqdm\n",
    "\n",
    "wahab_labels = (\n",
    "    jaw_config[\"rdsf_dir\"]\n",
    "    / pathlib.Path(\"1Felix and Rich make models/Training dataset Tiffs/Training set 1\")\n",
    ").glob(\"*.tif\")\n",
    "\n",
    "# Remove these ones, since the 3D tifs dont exist\n",
    "bad_labels = re.compile(r\"(351|401|420|441)\")\n",
    "wahab_labels = [label for label in wahab_labels if not bad_labels.search(label.name)]\n",
    "\n",
    "# Read the labels\n",
    "quadrate_labels = [tifffile.imread(path) for path in tqdm(wahab_labels)]\n",
    "quadrate_labels = [(l == 4) | (l == 5) for l in tqdm(quadrate_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b98b16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read in the images\n",
    "\"\"\"\n",
    "\n",
    "from fishjaw.util import files\n",
    "\n",
    "img_paths = [files.get_3d_tif(label_path) for label_path in wahab_labels]\n",
    "for p in img_paths:\n",
    "    assert p.exists()\n",
    "\n",
    "quadrate_imgs = [tifffile.imread(path) for path in tqdm(img_paths)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b1a9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the centre of the quadrates and crop the labels and images\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from scipy.ndimage import center_of_mass\n",
    "\n",
    "centroids = [\n",
    "    tuple(round(x) for x in center_of_mass(label)) for label in tqdm(quadrate_labels)  # type: ignore\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5448bcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fishjaw.images import transform\n",
    "\n",
    "window_size = transform.window_size(jaw_config)\n",
    "cropped_labels = [\n",
    "    transform.crop(l, c, window_size, centred=True)\n",
    "    for l, c in zip(tqdm(quadrate_labels), centroids)\n",
    "]\n",
    "cropped_quadrates = [\n",
    "    transform.crop(i, c, window_size, centred=True)\n",
    "    for i, c in zip(tqdm(quadrate_imgs), centroids)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd459fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot the jaws and labels just to check\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from fishjaw.visualisation import images_3d\n",
    "\n",
    "if debug_plots:\n",
    "    for img, label in zip(cropped_quadrates, cropped_labels):\n",
    "        images_3d.plot_slices(img, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbbfac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a dataloader for these\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torchio as tio\n",
    "from fishjaw.model import data\n",
    "\n",
    "\n",
    "# This is the size of the training data\n",
    "jaw_config[\"batch_size\"] = 10\n",
    "\n",
    "# Because we're in Jupyter\n",
    "jaw_config[\"num_workers\"] = 0\n",
    "\n",
    "# Turn them all into tio subjects first\n",
    "subjects = [\n",
    "    data.imgs2subject(img, label)\n",
    "    for img, label in zip(cropped_quadrates, cropped_labels)\n",
    "]\n",
    "\n",
    "train_subjects = tio.SubjectsDataset(\n",
    "    subjects[:10], transform=data._transforms(jaw_config[\"transforms\"])\n",
    ")\n",
    "val_subjects = tio.SubjectsDataset(\n",
    "    [subjects[-2]], transform=data._transforms(jaw_config[\"transforms\"])\n",
    ")\n",
    "test_subject = subjects[-1]\n",
    "\n",
    "quadrate_data = data.DataConfig(jaw_config, train_subjects, val_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2b7496",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot the first bit of trainin data just to visualise it\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if debug_plots:\n",
    "    for i, batch in enumerate(quadrate_data.train_data):\n",
    "        images = batch[tio.IMAGE][tio.DATA]\n",
    "        masks = batch[tio.LABEL][tio.DATA]\n",
    "        # Images per batch\n",
    "        for j, (image, mask) in enumerate(zip(images, masks)):\n",
    "            fig, _ = images_3d.plot_slices(\n",
    "                image.squeeze().numpy(), mask.squeeze().numpy()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b72a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot the ground truth for the test data\n",
    "\"\"\"\n",
    "\n",
    "_ = images_3d.plot_subject(test_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05491904",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train a model from scratch\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def train_new_model(\n",
    "    data_config: data.DataConfig,\n",
    ") -> tuple[\n",
    "    tuple[torch.nn.Module, list[list[float]], list[list[float]]], torch.optim.Optimizer\n",
    "]:\n",
    "    \"\"\"\n",
    "    Create a new model, train and return it\n",
    "\n",
    "    Returns the model, the training losses and the validation losses, and the optimiser\n",
    "\n",
    "    \"\"\"\n",
    "    # Create a model and optimiser\n",
    "    net = model.model(jaw_config[\"model_params\"])\n",
    "    net = net.to(jaw_config[\"device\"])\n",
    "    print(f\"Model loaded to {jaw_config['device']}\")\n",
    "\n",
    "    optimiser = model.optimiser(jaw_config, net)\n",
    "\n",
    "    # Define loss function\n",
    "    loss = model.lossfn(jaw_config)\n",
    "\n",
    "    train_config = model.TrainingConfig(\n",
    "        jaw_config[\"device\"],\n",
    "        jaw_config[\"epochs\"],\n",
    "        torch.optim.lr_scheduler.ExponentialLR(\n",
    "            optimiser, gamma=jaw_config[\"lr_lambda\"]\n",
    "        ),\n",
    "    )\n",
    "    return (\n",
    "        model.train(net, optimiser, loss, data_config, train_config),\n",
    "        optimiser,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdbe3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "jaw_config[\"epochs\"] = 650\n",
    "(net, train_losses, val_losses), optimiser = train_new_model(quadrate_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e11ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": net.state_dict(),\n",
    "        \"optimizer_state_dict\": optimiser.state_dict(),\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "    },\n",
    "    pathlib.Path(jaw_config[\"model_path\"].replace(\".pkl\", \"_quadrates.pth\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9a6c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot the training and validation losses\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from fishjaw.visualisation import training\n",
    "\n",
    "fig = training.plot_losses(train_losses, val_losses)\n",
    "fig.savefig(\"new_model_losses.png\")\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1461a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot the test subject\n",
    "\"\"\"\n",
    "\n",
    "fig = images_3d.plot_inference(\n",
    "    net,\n",
    "    test_subject,\n",
    "    patch_size=data.get_patch_size(jaw_config),\n",
    "    patch_overlap=(4, 4, 4),\n",
    "    activation=model.activation_name(jaw_config),\n",
    "    batch_size=jaw_config[\"batch_size\"],\n",
    ")\n",
    "fig.savefig(\"new_model_inference.png\")\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddfbe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now fine-tune the trained model on the testing data. it should perform better\n",
    "\"\"\"\n",
    "\n",
    "from monai.networks.nets.attentionunet import AttentionLayer\n",
    "\n",
    "\n",
    "def fine_tune_model(\n",
    "    data_config: data.DataConfig,\n",
    "    train_layers: str = list[int],\n",
    "    lr_multiplier: float = 0.1,  # Lower learning rate for fine-tuning\n",
    "    epochs_frozen: int = 150,  # Train with frozen layers\n",
    "    epochs_unfrozen: int = 50,  # Additional training with all layers\n",
    "    verbose: bool = True,\n",
    ") -> tuple[torch.nn.Module, list[list[float]], list[list[float]]]:\n",
    "    \"\"\"\n",
    "    Fine-tune a model on the provided data\n",
    "\n",
    "    :param train_layers: a list of integers: the layers to train\n",
    "    \"\"\"\n",
    "    for l in train_layers:\n",
    "        assert isinstance(l, int), f\"Expected int, got {type(l)}\"\n",
    "        assert 0 <= l <= 5\n",
    "\n",
    "    # Load the model from disk fresh so that we don't overwrite anything in memory\n",
    "    new_model = model.load_model(model_name)\n",
    "    net = new_model.load_model(set_eval=False)\n",
    "    net.to(jaw_config[\"device\"])\n",
    "\n",
    "    # Freeze all the parameters\n",
    "    for param in net.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Get the bits of the model\n",
    "    head = net.model[0]  # ConvBlock\n",
    "    encdec = net.model[1]  # The recursive AttentionLayer structure\n",
    "    final_conv = net.model[2]  # Final convolution\n",
    "\n",
    "    def unfreeze_attention_layers(\n",
    "        module: torch.nn.Module, current_depth: int, target_depths: list[int]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Recursively unfreeze AttentionLayer parameters at specified depths\n",
    "        \"\"\"\n",
    "        if isinstance(module, AttentionLayer):\n",
    "            if current_depth in target_depths:\n",
    "                if verbose:\n",
    "                    print(f\"Unfreezing AttentionLayer at depth {current_depth}\")\n",
    "                for name, param in module.named_parameters():\n",
    "                    if \"submodule\" not in name:\n",
    "                        param.requires_grad = True\n",
    "                        if verbose:\n",
    "                            print(\"  Unfreezing:\", name)\n",
    "\n",
    "            # Recurse into submodule\n",
    "            if hasattr(module, \"submodule\"):\n",
    "                unfreeze_attention_layers(\n",
    "                    module.submodule, current_depth + 1, target_depths\n",
    "                )\n",
    "\n",
    "        # Handle Sequential containers (like in submodule)\n",
    "        elif isinstance(module, torch.nn.Sequential):\n",
    "            for child in module.children():\n",
    "                unfreeze_attention_layers(child, current_depth, target_depths)\n",
    "\n",
    "    unfreeze_attention_layers(encdec, current_depth=0, target_depths=train_layers)\n",
    "    if verbose:\n",
    "        trainable_params = 0\n",
    "        total_params = 0\n",
    "        for name, param in net.named_parameters():\n",
    "            total_params += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_params += param.numel()\n",
    "                print(f\"Trainable: {name} ({param.numel()} params)\")\n",
    "\n",
    "        print(f\"Total trainable parameters: {trainable_params:,} / {total_params:,}\")\n",
    "\n",
    "    # Create a new optimiser that only updates the unfrozen layers\n",
    "    # Get the right optimiser from the config\n",
    "    # and set the learning rate to a lower value\n",
    "    optimiser = getattr(torch.optim, jaw_config[\"optimiser\"])(\n",
    "        (p for p in net.parameters() if p.requires_grad),\n",
    "        lr=jaw_config[\"learning_rate\"] * lr_multiplier,\n",
    "    )\n",
    "\n",
    "    # Create a loss function\n",
    "    loss = model.lossfn(jaw_config)\n",
    "\n",
    "    # Train the model with the frozen layers\n",
    "    train_config = model.TrainingConfig(\n",
    "        jaw_config[\"device\"],\n",
    "        epochs_frozen,\n",
    "        torch.optim.lr_scheduler.ExponentialLR(\n",
    "            optimiser, gamma=jaw_config[\"lr_lambda\"]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    print(f\"Training with selective freezing for {epochs_frozen} epochs...\")\n",
    "    return model.train(net, optimiser, loss, data_config, train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dc72b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "First demonstrate what the fine tuning looks like if we basically dont train it at all\n",
    "\"\"\"\n",
    "\n",
    "fine_tuned_model, fine_tune_train_losses, fine_tune_val_losses = fine_tune_model(\n",
    "    quadrate_data,\n",
    "    train_layers=[1, 2],\n",
    "    lr_multiplier=2.0,\n",
    "    epochs_frozen=2,\n",
    "    epochs_unfrozen=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d1b58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = images_3d.plot_inference(\n",
    "    fine_tuned_model,\n",
    "    test_subject,\n",
    "    patch_size=data.get_patch_size(jaw_config),\n",
    "    patch_overlap=(4, 4, 4),\n",
    "    activation=model.activation_name(jaw_config),\n",
    "    batch_size=jaw_config[\"batch_size\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00e1955",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now train it more properly\n",
    "\"\"\"\n",
    "\n",
    "fine_tuned_model, fine_tune_train_losses, fine_tune_val_losses = fine_tune_model(\n",
    "    quadrate_data,\n",
    "    train_layers=[1, 2],\n",
    "    lr_multiplier=2.0,\n",
    "    epochs_frozen=100,\n",
    "    epochs_unfrozen=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddc0f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fishjaw.visualisation import training\n",
    "\n",
    "training.plot_losses(fine_tune_train_losses, fine_tune_val_losses)\n",
    "_ = images_3d.plot_inference(\n",
    "    fine_tuned_model,\n",
    "    test_subject,\n",
    "    patch_size=data.get_patch_size(jaw_config),\n",
    "    patch_overlap=(4, 4, 4),\n",
    "    activation=model.activation_name(jaw_config),\n",
    "    batch_size=jaw_config[\"batch_size\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92328220",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We might want to compare the weights of the original model and the fine-tuned model, to see where the changes are\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_weight_deltas(\n",
    "    model_before, model_after\n",
    ") -> tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Get the difference in weights between two models, and the original weights\n",
    "    \"\"\"\n",
    "    deltas = {}\n",
    "    orig_weights = {}\n",
    "    for (name1, param1), (name2, param2) in zip(\n",
    "        model_before.named_parameters(), model_after.named_parameters()\n",
    "    ):\n",
    "        assert name1 == name2, f\"Names do not match: {name1} != {name2}\"\n",
    "\n",
    "        if param1.requires_grad and \"num_batches_tracked\" not in name1:\n",
    "            delta = param2.data - param1.data\n",
    "            deltas[name1] = delta\n",
    "\n",
    "            orig_weights[name1] = param1.data\n",
    "    return deltas, orig_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3388eb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot histograms of all of the weight deltas\n",
    "\"\"\"\n",
    "\n",
    "deltas, orig_weights = get_weight_deltas(\n",
    "    jaw_model.load_model().to(jaw_config[\"device\"]), fine_tuned_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151b3593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Only do this sometimes because it's a really big image\n",
    "if debug_plots:\n",
    "    bins = np.linspace(\n",
    "        torch.min(torch.cat(list(d.flatten() for d in deltas.values()))).item(),\n",
    "        torch.max(torch.cat(list(d.flatten() for d in deltas.values()))).item(),\n",
    "        100,\n",
    "    )\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        len(deltas) // 4, 4, figsize=(8, len(deltas) // 4 * 2), sharey=True\n",
    "    )\n",
    "    for axis, (name, delta) in zip(axes.flatten(), tqdm(deltas.items())):\n",
    "        axis.hist(delta.flatten().cpu().numpy(), bins=bins, density=True)\n",
    "        axis.set_title(\"\\n\".join(textwrap.wrap(name, 30)), fontsize=8)\n",
    "        # axis.set_yscale(\"log\")\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b19e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We want a way to isolate each type of weight (conv, psi, merge, etc)\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "weight_type_regex = {\n",
    "    \"down_conv_0_weight\": r\".*conv.0.conv.weight\",\n",
    "    \"down_conv_0_bias\": r\".*conv.0.conv.bias\",\n",
    "    \"down_conv_1_weight\": r\".*conv.1.conv.weight\",\n",
    "    \"down_conv_1_bias\": r\".*conv.1.conv.bias\",\n",
    "    \"down_adn_0_weight\": r\".*conv.0.adn.N.weight\",\n",
    "    \"down_adn_0_bias\": r\".*conv.0.adn.N.bias\",\n",
    "    \"down_adn_1_weight\": r\".*conv.1.adn.N.weight\",\n",
    "    \"down_adn_1_bias\": r\".*conv.1.adn.N.bias\",\n",
    "    \"attention_wg_0_weight\": r\".*attention.W_g.0.conv.weight\",\n",
    "    \"attention_wg_0_bias\": r\".*attention.W_g.0.conv.bias\",\n",
    "    \"attention_wg_1_weight\": r\".*attention.W_g.1.weight\",\n",
    "    \"attention_wg_1_bias\": r\".*attention.W_g.1.bias\",\n",
    "    \"attention_wx_0_weight\": r\".*attention.W_x.0.conv.weight\",\n",
    "    \"attention_wx_0_bias\": r\".*attention.W_x.0.conv.bias\",\n",
    "    \"attention_wx_1_weight\": r\".*attention.W_x.1.weight\",\n",
    "    \"attention_wx_1_bias\": r\".*attention.W_x.1.bias\",\n",
    "    \"attention_psi_0_weight\": r\".*attention.psi.0.conv.weight\",\n",
    "    \"attention_psi_0_bias\": r\".*attention.psi.0.conv.bias\",\n",
    "    \"attention_psi_1_weight\": r\".*attention.psi.1.weight\",\n",
    "    \"attention_psi_1_bias\": r\".*attention.psi.1.bias\",\n",
    "    \"upconv_weight\": r\".*upconv.up.conv.weight\",\n",
    "    \"upconv_bias\": r\".*upconv.up.conv.bias\",\n",
    "    \"upconv_adn_weight\": r\".*upconv.up.adn.N.weight\",\n",
    "    \"upconv_adn_bias\": r\".*upconv.up.adn.N.bias\",\n",
    "    \"merge_weight\": r\".*merge.conv.weight\",\n",
    "    \"merge_bias\": r\".*merge.conv.bias\",\n",
    "    \"merge_adn\": r\".*merge.adn.A.weight\",\n",
    "}\n",
    "\n",
    "\n",
    "if debug_plots:\n",
    "    for name, regex in weight_type_regex.items():\n",
    "        total = 0\n",
    "        for k in deltas.keys():\n",
    "            if re.search(regex, k):\n",
    "                total += 1\n",
    "        print(f\"{name}: {total} matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8f2c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make a u-net style diagram, where each histogram is in a different part of the u-net, and the skip connections are drawn as arrows\n",
    "\n",
    "As much as I love this, it's ridiculous\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "\n",
    "\n",
    "def _draw_skip_arrows(fig: plt.Figure, axes: dict) -> None:\n",
    "    \"\"\"\n",
    "    draw arrows\n",
    "    \"\"\"\n",
    "    # Define the skip connections (encoder to bottleneck to decoder)\n",
    "    skip_connections = [\n",
    "        (\"A\", \"a\", \"K\"),  # Encoder level 1 → Bottleneck → Decoder level 1\n",
    "        (\"B\", \"b\", \"J\"),  # Encoder level 2 → Bottleneck → Decoder level 2\n",
    "        (\"C\", \"c\", \"I\"),  # Encoder level 3 → Bottleneck → Decoder level 3\n",
    "        (\"D\", \"d\", \"H\"),  # Encoder level 4 → Bottleneck → Decoder level 4\n",
    "        (\"E\", \"e\", \"G\"),  # Encoder level 5 → Bottleneck → Decoder level 5\n",
    "    ]\n",
    "\n",
    "    # Add arrows for skip connections\n",
    "    arrow_params = dict(\n",
    "        connectionstyle=\"arc3,rad=-0.3\",\n",
    "        arrowstyle=\"simple,head_length=5,head_width=5\",\n",
    "        linewidth=0.5,\n",
    "        transform=fig.transFigure,\n",
    "        color=\"k\",\n",
    "    )\n",
    "    for encoder, bottleneck, decoder in skip_connections:\n",
    "        # Get positions of axes\n",
    "        encoder_pos = axes[encoder].get_position()\n",
    "        attn_pos = axes[bottleneck].get_position()\n",
    "        decoder_pos = axes[decoder].get_position()\n",
    "\n",
    "        # Calculate arrow coordinates\n",
    "        # Encoder to bottleneck arrow\n",
    "        x1 = encoder_pos.x1  # Right side of encoder\n",
    "        y1 = encoder_pos.y0 + 0.75 * encoder_pos.height  # 3/4 up\n",
    "        x2 = attn_pos.x0  # Left side of bottleneck\n",
    "        y2 = attn_pos.y0 + 0.5 * attn_pos.height  # Middle\n",
    "\n",
    "        # Draw arrow\n",
    "        fig.patches.extend([FancyArrowPatch((x1, y1), (x2, y2), **arrow_params)])\n",
    "\n",
    "        # Bottleneck to decoder arrow\n",
    "        x1 = attn_pos.x1  # Right side of bottleneck\n",
    "        y1 = attn_pos.y0 + 0.5 * attn_pos.height  # Middle\n",
    "        x2 = decoder_pos.x0  # Left side of decoder\n",
    "        y2 = decoder_pos.y0 + 0.75 * decoder_pos.height  # 3/4 up\n",
    "\n",
    "        fig.patches.extend([FancyArrowPatch((x1, y1), (x2, y2), **arrow_params)])\n",
    "\n",
    "\n",
    "def unet_hists(weight_type: str) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Draw histograms in a u-net shape for weights matching the given pattern\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the layer names for the given weight type\n",
    "    pattern = weight_type_regex[weight_type]\n",
    "    names = []\n",
    "    for k in deltas.keys():\n",
    "        if re.search(pattern, k):\n",
    "            names.append(k)\n",
    "\n",
    "    fig, axes = plt.subplot_mosaic(\n",
    "        \"\"\"\n",
    "        AA.........aKK\n",
    "        AA..........KK\n",
    "        .BB.......bJJ.\n",
    "        .BB........JJ.\n",
    "        ..CC.....cII..\n",
    "        ..CC......II..\n",
    "        ...DD...dHH...\n",
    "        ...DD....HH...\n",
    "        ....EE.eGG....\n",
    "        ....EE..GG....\n",
    "        ......FF......\n",
    "        ......FF......\n",
    "        \"\"\",\n",
    "        figsize=(15, 15),\n",
    "    )\n",
    "\n",
    "    _draw_skip_arrows(fig, axes)\n",
    "\n",
    "    # Choose the axes to plot on\n",
    "    down_axes = \"ABCDEF\"\n",
    "    up_axes = \"GHIJK\"\n",
    "    attn_axes = \"abcde\"\n",
    "\n",
    "    if \"upconv\" in weight_type or \"merge\" in weight_type:\n",
    "        plot_axes = {k: axes[k] for k in up_axes}\n",
    "    elif \"attention\" in weight_type:\n",
    "        plot_axes = {k: axes[k] for k in attn_axes}\n",
    "    else:\n",
    "        assert \"down\" in weight_type\n",
    "        plot_axes = {k: axes[k] for k in down_axes}\n",
    "\n",
    "    deltas_ = [deltas[k] for k in names]\n",
    "    max_ = max([d.abs().max().cpu() for d in deltas_])\n",
    "    bins = np.linspace(-max_, max_, 100)\n",
    "    for name, delta, axis in zip(names, deltas_, plot_axes.values()):\n",
    "        # Get the delta\n",
    "        delta = delta.flatten().cpu().numpy()\n",
    "\n",
    "        # Plot the histogram\n",
    "        axis.hist(delta, bins=bins, density=True)\n",
    "        axis.set_yscale(\"log\")\n",
    "\n",
    "        axis.axvline(\n",
    "            0,\n",
    "            color=\"k\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=0.5,\n",
    "        )\n",
    "\n",
    "    # Remove axis ticks on the unused axes\n",
    "    for axis in axes.values():\n",
    "        if axis not in plot_axes.values():\n",
    "            axis.set_xticks([])\n",
    "            axis.set_yticks([])\n",
    "\n",
    "    fig.suptitle(f\"$\\Delta${weight_type}\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb7a868",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Boxplots are a more sensible way to visualise things, I think\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def boxplots(deltas: dict) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Make a boxplot with the weight deltas for a given weight type\n",
    "    \"\"\"\n",
    "\n",
    "    # There are 27 weight types, I think\n",
    "    fig, axes = plt.subplots(9, 3, figsize=(9, 27), sharex=True, sharey=True)\n",
    "\n",
    "    for axis, weight_type in zip(\n",
    "        tqdm(axes.flatten()), weight_type_regex.keys(), strict=True\n",
    "    ):\n",
    "        pattern = weight_type_regex[weight_type]\n",
    "        names = []\n",
    "        for k in deltas.keys():\n",
    "            if re.search(pattern, k):\n",
    "                names.append(k)\n",
    "\n",
    "        for i, d in enumerate([deltas[k] for k in names]):\n",
    "            axis.boxplot(\n",
    "                d.flatten().cpu().numpy(),\n",
    "                positions=[i],\n",
    "                widths=0.5,\n",
    "                vert=True,\n",
    "                patch_artist=True,\n",
    "            )\n",
    "            axis.set_title(weight_type)\n",
    "            axis.axhline(\n",
    "                0,\n",
    "                color=\"k\",\n",
    "                linestyle=\"--\",\n",
    "                linewidth=0.5,\n",
    "            )\n",
    "\n",
    "    fig.suptitle(\"$\\Delta$ weight\")\n",
    "    fig.supxlabel(\"Layer\")\n",
    "    fig.supylabel(\"Change in weight\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4113f680",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = boxplots(deltas)\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9692ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train all the layers\n",
    "\"\"\"\n",
    "\n",
    "fine_tuned_model, fine_tune_train_losses, fine_tune_val_losses = fine_tune_model(\n",
    "    quadrate_data,\n",
    "    train_layers=[0, 1, 2, 3, 4],\n",
    "    lr_multiplier=0.5,\n",
    "    epochs_frozen=200,\n",
    "    epochs_unfrozen=50,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "training.plot_losses(fine_tune_train_losses, fine_tune_val_losses)\n",
    "_ = images_3d.plot_inference(\n",
    "    fine_tuned_model,\n",
    "    test_subject,\n",
    "    patch_size=data.get_patch_size(jaw_config),\n",
    "    patch_overlap=(4, 4, 4),\n",
    "    activation=model.activation_name(jaw_config),\n",
    "    batch_size=jaw_config[\"batch_size\"],\n",
    ")\n",
    "\n",
    "fig = boxplots(\n",
    "    get_weight_deltas(\n",
    "        jaw_model.load_model().to(jaw_config[\"device\"]), fine_tuned_model\n",
    "    )[0]\n",
    ")\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf615e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Do the above with every two-layer combination, to see if we can get a better result\n",
    "\"\"\"\n",
    "\n",
    "from itertools import combinations\n",
    "from fishjaw.util import files\n",
    "\n",
    "out_dir = files.script_out_dir() / \"fine_tune_quadrates\"\n",
    "if not out_dir.exists():\n",
    "    out_dir = pathlib.Path(\"script_output\")\n",
    "    out_dir.mkdir(parents=True)\n",
    "\n",
    "layers = [0, 1, 2, 3, 4]\n",
    "all_combinations = [\n",
    "    list(combo) for i in range(1, len(layers) + 1) for combo in combinations(layers, i)\n",
    "]\n",
    "\n",
    "for train_layers in tqdm(all_combinations):\n",
    "    print(f\"Training layers: {train_layers}\")\n",
    "    this_dir = out_dir / \"_\".join(map(str, train_layers))\n",
    "    if not this_dir.exists():\n",
    "        this_dir.mkdir(parents=True)\n",
    "\n",
    "    fine_tuned_model, fine_tune_train_losses, fine_tune_val_losses = fine_tune_model(\n",
    "        quadrate_data,\n",
    "        train_layers=train_layers,\n",
    "        lr_multiplier=0.5,\n",
    "        epochs_frozen=200,\n",
    "        epochs_unfrozen=50,\n",
    "        verbose=False,\n",
    "    )\n",
    "    fig = training.plot_losses(fine_tune_train_losses, fine_tune_val_losses)\n",
    "    fig.savefig(this_dir / f\"losses.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig = images_3d.plot_inference(\n",
    "        fine_tuned_model,\n",
    "        test_subject,\n",
    "        patch_size=data.get_patch_size(jaw_config),\n",
    "        patch_overlap=(4, 4, 4),\n",
    "        activation=model.activation_name(jaw_config),\n",
    "        batch_size=jaw_config[\"batch_size\"],\n",
    "    )\n",
    "    fig.savefig(this_dir / f\"inference.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    fig = boxplots(\n",
    "        get_weight_deltas(\n",
    "            jaw_model.load_model().to(jaw_config[\"device\"]), fine_tuned_model\n",
    "        )[0]\n",
    "    )\n",
    "    fig.savefig(this_dir / f\"weight_deltas.png\")\n",
    "    plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zebrafish_jaw_segmentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
