{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfc5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add the zebrafish jaw lib to python path\n",
    "\"\"\"\n",
    "\n",
    "import os, sys\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\"\"), \"..\", \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365b25cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_plots = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30bdda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The first thing we'll do is read a model from disk - we want to access its config file\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from fishjaw.model import model\n",
    "\n",
    "model_name = \"attempt_3.pkl\"\n",
    "jaw_model = model.load_model(\"attempt_3.pkl\")\n",
    "jaw_config = jaw_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426f0b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We'll fine tune on some quadrates that Wahab segmented - we'll want to read these from the RDSF and crop to the right region of interest\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import pathlib\n",
    "\n",
    "import tifffile\n",
    "from tqdm import tqdm\n",
    "\n",
    "wahab_labels = (\n",
    "    jaw_config[\"rdsf_dir\"]\n",
    "    / pathlib.Path(\"1Felix and Rich make models/Training dataset Tiffs/Training set 1\")\n",
    ").glob(\"*.tif\")\n",
    "\n",
    "# Remove these ones, since the 3D tifs dont exist\n",
    "bad_labels = re.compile(r\"(351|401|420|441)\")\n",
    "wahab_labels = [label for label in wahab_labels if not bad_labels.search(label.name)]\n",
    "\n",
    "# Read the labels\n",
    "quadrate_labels = [tifffile.imread(path) for path in tqdm(wahab_labels)]\n",
    "quadrate_labels = [(l == 4) | (l == 5) for l in tqdm(quadrate_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b98b16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read in the images\n",
    "\"\"\"\n",
    "\n",
    "from fishjaw.util import files\n",
    "\n",
    "img_paths = [files.get_3d_tif(label_path) for label_path in wahab_labels]\n",
    "for p in img_paths:\n",
    "    assert p.exists()\n",
    "\n",
    "quadrate_imgs = [tifffile.imread(path) for path in tqdm(img_paths)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b1a9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find the centre of the quadrates and crop the labels and images\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from scipy.ndimage import center_of_mass\n",
    "\n",
    "centroids = [\n",
    "    tuple(round(x) for x in center_of_mass(label)) for label in tqdm(quadrate_labels)  # type: ignore\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5448bcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fishjaw.images import transform\n",
    "\n",
    "window_size = transform.window_size(jaw_config)\n",
    "cropped_labels = [transform.crop(l, c, window_size, centred=True) for l, c in zip(tqdm(quadrate_labels), centroids)]\n",
    "cropped_quadrates = [transform.crop(i, c, window_size, centred=True) for i, c in zip(tqdm(quadrate_imgs), centroids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd459fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot the jaws and labels just to check\n",
    "\n",
    "\"\"\"\n",
    "from fishjaw.visualisation import images_3d\n",
    "\n",
    "if debug_plots:\n",
    "    for img, label in zip(cropped_quadrates, cropped_labels):\n",
    "        images_3d.plot_slices(img, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbbfac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a dataloader for these\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torchio as tio\n",
    "from fishjaw.model import data\n",
    "\n",
    "\n",
    "# This is the size of the training data\n",
    "jaw_config[\"batch_size\"] = 10\n",
    "\n",
    "# Because we're in Jupyter\n",
    "jaw_config[\"num_workers\"] = 0\n",
    "\n",
    "# Turn them all into tio subjects first\n",
    "subjects = [\n",
    "    data.imgs2subject(img, label)\n",
    "    for img, label in zip(cropped_quadrates, cropped_labels)\n",
    "]\n",
    "\n",
    "train_subjects = tio.SubjectsDataset(\n",
    "    subjects[:10], transform=data._transforms(jaw_config[\"transforms\"])\n",
    ")\n",
    "val_subjects = tio.SubjectsDataset(\n",
    "    [subjects[-2]], transform=data._transforms(jaw_config[\"transforms\"])\n",
    ")\n",
    "test_subject = subjects[-1]\n",
    "\n",
    "quadrate_data = data.DataConfig(jaw_config, train_subjects, val_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2b7496",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot the first bit of trainin data just to visualise it\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if debug_plots:\n",
    "    for i, batch in enumerate(quadrate_data.train_data):\n",
    "        images = batch[tio.IMAGE][tio.DATA]\n",
    "        masks = batch[tio.LABEL][tio.DATA]\n",
    "        # Images per batch\n",
    "        for j, (image, mask) in enumerate(zip(images, masks)):\n",
    "            fig, _ = images_3d.plot_slices(\n",
    "                image.squeeze().numpy(), mask.squeeze().numpy()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b72a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot the ground truth for the test data\n",
    "\"\"\"\n",
    "_ = images_3d.plot_subject(test_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05491904",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train a model from scratch\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def train_new_model(\n",
    "    data_config: data.DataConfig,\n",
    ") -> tuple[\n",
    "    tuple[torch.nn.Module, list[list[float]], list[list[float]]], torch.optim.Optimizer\n",
    "]:\n",
    "    \"\"\"\n",
    "    Create a new model, train and return it\n",
    "\n",
    "    Returns the model, the training losses and the validation losses, and the optimiser\n",
    "\n",
    "    \"\"\"\n",
    "    # Create a model and optimiser\n",
    "    net = model.model(jaw_config[\"model_params\"])\n",
    "    net = net.to(jaw_config[\"device\"])\n",
    "    print(f\"Model loaded to {jaw_config['device']}\")\n",
    "\n",
    "    optimiser = model.optimiser(jaw_config, net)\n",
    "\n",
    "    # Define loss function\n",
    "    loss = model.lossfn(jaw_config)\n",
    "\n",
    "    train_config = model.TrainingConfig(\n",
    "        jaw_config[\"device\"],\n",
    "        jaw_config[\"epochs\"],\n",
    "        torch.optim.lr_scheduler.ExponentialLR(\n",
    "            optimiser, gamma=jaw_config[\"lr_lambda\"]\n",
    "        ),\n",
    "    )\n",
    "    return (\n",
    "        model.train(net, optimiser, loss, data_config, train_config),\n",
    "        optimiser,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdbe3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "jaw_config[\"epochs\"] = 450\n",
    "(net, train_losses, val_losses), optimiser = train_new_model(quadrate_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9a6c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot the training and validation losses\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from fishjaw.visualisation import training\n",
    "\n",
    "fig = training.plot_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1461a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot the test subject\n",
    "\"\"\"\n",
    "\n",
    "fig = images_3d.plot_inference(\n",
    "    net,\n",
    "    test_subject,\n",
    "    patch_size=data.get_patch_size(jaw_config),\n",
    "    patch_overlap=(4, 4, 4),\n",
    "    activation=model.activation_name(jaw_config),\n",
    "    batch_size=jaw_config[\"batch_size\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddfbe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now fine-tune the trained model on the testing data. it should perform better\n",
    "\"\"\"\n",
    "\n",
    "from monai.networks.nets.attentionunet import AttentionBlock\n",
    "\n",
    "\n",
    "def fine_tune_model(\n",
    "    data_config: data.DataConfig,\n",
    "    train_layers: str = \"1,2\",\n",
    "    lr_multiplier: float = 0.1,  # Lower learning rate for fine-tuning\n",
    "    epochs_frozen: int = 150,  # Train with frozen layers\n",
    "    epochs_unfrozen: int = 50,  # Additional training with all layers\n",
    "    verbose: bool = True,\n",
    ") -> tuple[torch.nn.Module, list[list[float]], list[list[float]]]:\n",
    "    \"\"\"\n",
    "    Fine-tune a model on the provided data\n",
    "\n",
    "    :param freeze_layers: The layers to freeze, either:\n",
    "        - \"all\": all layers are trainable\n",
    "        - a comma-separated list of integers: the attention mechanisms layers to freeze\n",
    "\n",
    "    \"\"\"\n",
    "    match train_layers:\n",
    "        case \"all\":\n",
    "            train_all = True\n",
    "        case _:\n",
    "            train_all = False\n",
    "            train_layers = [int(x) for x in train_layers.split(\",\")]\n",
    "\n",
    "    # Load the model from disk fresh so that we don't overwrite anything in memory\n",
    "    new_model = model.load_model(model_name)\n",
    "    net = new_model.load_model(set_eval=False)\n",
    "    net.to(jaw_config[\"device\"])\n",
    "\n",
    "    if train_all:\n",
    "        ...\n",
    "    else:\n",
    "        # Freeze all the parameters\n",
    "        for param in net.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Unfreeze the morphology layers\n",
    "        attention_block_index = 0\n",
    "        for module in net.modules():\n",
    "            if isinstance(module, AttentionBlock):\n",
    "                for name, submodule in module.named_children():\n",
    "                    if name == \"psi\" and attention_block_index in train_layers:\n",
    "                        if verbose:\n",
    "                            print(\"unfreezing\")\n",
    "                        for param in submodule.parameters():\n",
    "                            param.requires_grad = True\n",
    "                    elif name == \"psi\":\n",
    "                        if verbose:\n",
    "                            print(\"not unfreezing\")\n",
    "                attention_block_index += 1\n",
    "\n",
    "        if verbose:\n",
    "            for name, param in net.named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    print(f\"Trainable: {name}\")\n",
    "\n",
    "    # Create a new optimiser that only updates the unfrozen layers\n",
    "    # Get the right optimiser from the config\n",
    "    # and set the learning rate to a lower value\n",
    "    optimiser = getattr(torch.optim, jaw_config[\"optimiser\"])(\n",
    "        (p for p in net.parameters() if p.requires_grad),\n",
    "        lr=jaw_config[\"learning_rate\"] * lr_multiplier,\n",
    "    )\n",
    "\n",
    "    # Create a loss function\n",
    "    loss = model.lossfn(jaw_config)\n",
    "\n",
    "    # Train the model with the frozen layers\n",
    "    train_config = model.TrainingConfig(\n",
    "        jaw_config[\"device\"],\n",
    "        epochs_frozen,\n",
    "        torch.optim.lr_scheduler.ExponentialLR(\n",
    "            optimiser, gamma=jaw_config[\"lr_lambda\"]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    print(f\"Training with selective freezing for {epochs_frozen} epochs...\")\n",
    "    return model.train(net, optimiser, loss, data_config, train_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dc72b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "First demonstrate what the fine tuning looks like if we basically dont train it at all\n",
    "\"\"\"\n",
    "\n",
    "fine_tuned_model, fine_tune_train_losses, fine_tune_val_losses = fine_tune_model(\n",
    "    quadrate_data,\n",
    "    train_layers=\"all\",\n",
    "    lr_multiplier=2.0,\n",
    "    epochs_frozen=2,\n",
    "    epochs_unfrozen=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d1b58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = images_3d.plot_inference(\n",
    "    fine_tuned_model,\n",
    "    test_subject,\n",
    "    patch_size=data.get_patch_size(jaw_config),\n",
    "    patch_overlap=(4, 4, 4),\n",
    "    activation=model.activation_name(jaw_config),\n",
    "    batch_size=jaw_config[\"batch_size\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00e1955",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now train it more properly\n",
    "\"\"\"\n",
    "\n",
    "fine_tuned_model, fine_tune_train_losses, fine_tune_val_losses = fine_tune_model(\n",
    "    quadrate_data,\n",
    "    train_layers=\"all\",\n",
    "    lr_multiplier=2.0,\n",
    "    epochs_frozen=100,\n",
    "    epochs_unfrozen=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddc0f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fishjaw.visualisation import training\n",
    "\n",
    "training.plot_losses(fine_tune_train_losses, fine_tune_val_losses)\n",
    "_ = images_3d.plot_inference(\n",
    "    fine_tuned_model,\n",
    "    test_subject,\n",
    "    patch_size=data.get_patch_size(jaw_config),\n",
    "    patch_overlap=(4, 4, 4),\n",
    "    activation=model.activation_name(jaw_config),\n",
    "    batch_size=jaw_config[\"batch_size\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92328220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight_deltas(\n",
    "    model_before, model_after\n",
    ") -> tuple[dict[str, torch.Tensor], dict[str, torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Get the difference in weights between two models, and the original weights\n",
    "    \"\"\"\n",
    "    deltas = {}\n",
    "    orig_weights = {}\n",
    "    for (name1, param1), (name2, param2) in zip(\n",
    "        model_before.named_parameters(), model_after.named_parameters()\n",
    "    ):\n",
    "        assert name1 == name2, f\"Names do not match: {name1} != {name2}\"\n",
    "\n",
    "        if param1.requires_grad and \"num_batches_tracked\" not in name1:\n",
    "            delta = param2.data - param1.data\n",
    "            deltas[name1] = delta\n",
    "\n",
    "            orig_weights[name1] = param1.data\n",
    "    return deltas, orig_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3388eb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot histograms\n",
    "\"\"\"\n",
    "deltas, orig_weights = get_weight_deltas(jaw_model.load_model().to(jaw_config[\"device\"]), fine_tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151b3593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bins = np.linspace(\n",
    "    torch.min(torch.cat(list(d.flatten() for d in deltas.values()))).item(),\n",
    "    torch.max(torch.cat(list(d.flatten() for d in deltas.values()))).item(),\n",
    "    100,\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    len(deltas) // 4, 4, figsize=(8, len(deltas) // 4 * 2), sharey=True\n",
    ")\n",
    "for axis, (name, delta) in zip(axes.flatten(), tqdm(deltas.items())):\n",
    "    axis.hist(delta.flatten().cpu().numpy(), bins=bins, density=True)\n",
    "    axis.set_title(\"\\n\".join(textwrap.wrap(name, 30)), fontsize=8)\n",
    "    # axis.set_yscale(\"log\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b19e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We want a way to isolate each type of weight (conv, psi, merge, etc)\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "weight_type_regex = {\n",
    "    \"down_conv_0_weight\": r\".*conv.0.conv.weight\",\n",
    "    \"down_conv_0_bias\": r\".*conv.0.conv.bias\",\n",
    "    \"down_conv_1_weight\": r\".*conv.1.conv.weight\",\n",
    "    \"down_conv_1_bias\": r\".*conv.1.conv.bias\",\n",
    "    \"down_adn_0_weight\": r\".*conv.0.adn.N.weight\",\n",
    "    \"down_adn_0_bias\": r\".*conv.0.adn.N.bias\",\n",
    "    \"down_adn_1_weight\": r\".*conv.1.adn.N.weight\",\n",
    "    \"down_adn_1_bias\": r\".*conv.1.adn.N.bias\",\n",
    "    \"attention_wg_0_weight\": r\".*attention.W_g.0.conv.weight\",\n",
    "    \"attention_wg_0_bias\": r\".*attention.W_g.0.conv.bias\",\n",
    "    \"attention_wg_1_weight\": r\".*attention.W_g.1.weight\",\n",
    "    \"attention_wg_1_bias\": r\".*attention.W_g.1.bias\",\n",
    "    \"attention_wx_0_weight\": r\".*attention.W_x.0.conv.weight\",\n",
    "    \"attention_wx_0_bias\": r\".*attention.W_x.0.conv.bias\",\n",
    "    \"attention_wx_1_weight\": r\".*attention.W_x.1.weight\",\n",
    "    \"attention_wx_1_bias\": r\".*attention.W_x.1.bias\",\n",
    "    \"attention_psi_0_weight\": r\".*attention.psi.0.conv.weight\",\n",
    "    \"attention_psi_0_bias\": r\".*attention.psi.0.conv.bias\",\n",
    "    \"attention_psi_1_weight\": r\".*attention.psi.1.weight\",\n",
    "    \"attention_psi_1_bias\": r\".*attention.psi.1.bias\",\n",
    "    \"upconv_weight\": r\".*upconv.up.conv.weight\",\n",
    "    \"upconv_bias\": r\".*upconv.up.conv.bias\",\n",
    "    \"upconv_adn_weight\": r\".*upconv.up.adn.N.weight\",\n",
    "    \"upconv_adn_bias\": r\".*upconv.up.adn.N.bias\", \n",
    "    \"merge_weight\": r\".*merge.conv.weight\",\n",
    "    \"merge_bias\": r\".*merge.conv.bias\",\n",
    "    \"merge_adn\": r\".*merge.adn.A.weight\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8f2c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Make a u-net style diagram\n",
    "\"\"\"\n",
    "\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "\n",
    "\n",
    "def _draw_arrows(fig: plt.Figure, axes: dict) -> None:\n",
    "    \"\"\"\n",
    "    draw arrows\n",
    "    \"\"\"\n",
    "    # Define the skip connections (encoder to bottleneck to decoder)\n",
    "    skip_connections = [\n",
    "        (\"A\", \"a\", \"K\"),  # Encoder level 1 → Bottleneck → Decoder level 1\n",
    "        (\"B\", \"b\", \"J\"),  # Encoder level 2 → Bottleneck → Decoder level 2\n",
    "        (\"C\", \"c\", \"I\"),  # Encoder level 3 → Bottleneck → Decoder level 3\n",
    "        (\"D\", \"d\", \"H\"),  # Encoder level 4 → Bottleneck → Decoder level 4\n",
    "        (\"E\", \"e\", \"G\"),  # Encoder level 5 → Bottleneck → Decoder level 5\n",
    "    ]\n",
    "\n",
    "    # Add arrows for skip connections\n",
    "    arrow_params = dict(\n",
    "        connectionstyle=\"arc3,rad=-0.3\",\n",
    "        arrowstyle=\"simple,head_length=5,head_width=5\",\n",
    "        linewidth=0.5,\n",
    "        transform=fig.transFigure,\n",
    "        color=\"k\",\n",
    "    )\n",
    "    for encoder, bottleneck, decoder in skip_connections:\n",
    "        # Get positions of axes\n",
    "        encoder_pos = axes[encoder].get_position()\n",
    "        attn_pos = axes[bottleneck].get_position()\n",
    "        decoder_pos = axes[decoder].get_position()\n",
    "\n",
    "        # Calculate arrow coordinates\n",
    "        # Encoder to bottleneck arrow\n",
    "        x1 = encoder_pos.x1  # Right side of encoder\n",
    "        y1 = encoder_pos.y0 + 0.75 * encoder_pos.height  # 3/4 up\n",
    "        x2 = attn_pos.x0  # Left side of bottleneck\n",
    "        y2 = attn_pos.y0 + 0.5 * attn_pos.height  # Middle\n",
    "\n",
    "        # Draw arrow\n",
    "        fig.patches.extend([FancyArrowPatch((x1, y1), (x2, y2), **arrow_params)])\n",
    "\n",
    "        # Bottleneck to decoder arrow\n",
    "        x1 = attn_pos.x1  # Right side of bottleneck\n",
    "        y1 = attn_pos.y0 + 0.5 * attn_pos.height  # Middle\n",
    "        x2 = decoder_pos.x0  # Left side of decoder\n",
    "        y2 = decoder_pos.y0 + 0.75 * decoder_pos.height  # 3/4 up\n",
    "\n",
    "        fig.patches.extend([FancyArrowPatch((x1, y1), (x2, y2), **arrow_params)])\n",
    "\n",
    "\n",
    "def unet_hists(pattern: str) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Draw histograms in a u-net shape for weights matching the given pattern\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplot_mosaic(\n",
    "        \"\"\"\n",
    "        AA.........aKK\n",
    "        AA..........KK\n",
    "        .BB.......bJJ.\n",
    "        .BB........JJ.\n",
    "        ..CC.....cII..\n",
    "        ..CC......II..\n",
    "        ...DD...dHH...\n",
    "        ...DD....HH...\n",
    "        ....EE.eGG....\n",
    "        ....EE..GG....\n",
    "        ......FF......\n",
    "        ......FF......\n",
    "        \"\"\",\n",
    "        figsize=(10, 10),\n",
    "    )\n",
    "\n",
    "    _draw_arrows(fig, axes)\n",
    "\n",
    "    for axis in axes.values():\n",
    "        axis.set_xticks([])\n",
    "        axis.set_yticks([])\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246f95e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = unet_hists(weight_type_regex[\"down_conv_1_weight\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zebrafish_jaw_segmentation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
