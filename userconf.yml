# User-defined configuration file; things here depend on where your files are etc.

# Where the RDSF is mounted - for reading the data + creating DICOMs
# The create_dicoms.py script creates these
rdsf_dir: "/home/mh19137/zebrafish_rdsf/"

# Where the DICOMs are stored - this is where the training data lives
dicom_dir: "/home/mh19137/zebrafish_jaw_segmentation/dicoms/"

# Optimiser options
optimiser: "Adam"  # Must be one of the torch.optim optimisers
learning_rate: 0.01

# Loss function options
loss: "monai.losses.TverskyLoss"
loss_options: {
  "include_background": false,
  "to_onehot_y": true,
  "alpha": 0.5,
  "beta": 0.5,
  "sigmoid": true,
}

# RNG seeds
# Note that this still doesn't guarantee reproducibility,
# since the dataloaders and probably some algorithms/other things have different sources
# of randomness
# But (hopefully) it should mean our model is initialised the same way each time
torch_seed: 0
test_train_seed: 1

# Options for the model that you might want to change
# These are sort of like meta-parameters so they're not in the model_params section but maybe they should be
device: "cuda"
window_size: "192,192,192"  # Comma-separated ZYX. Needs to be large enough to hold the whole jaw
patch_size: "160,160,160"  # Bigger holds more context, smaller is faster and allows for bigger batches
batch_size: 27
epochs: 10
lr_lambda: 0.9999 # Exponential decay factor (multiplicative with each epoch)

model_params:
  model_name: "monai.networks.nets.AttentionUnet"

  # Things you probably won't need to change - I just kept them here to keep all the params in one place
  spatial_dims: 3
  n_classes: 2  # n bones + background
  in_channels: 1  # Our images are greyscale

  # Things you might want to change
  # At the moment, I can't have more than 6 layers in the model because the receptive field
  # gets halved and ends up being an odd number, and then when we upsample we get a size mismatch
  n_layers: 6  # 6?
  n_initial_filters: 14  # 6 would be sensible
  kernel_size: 3
  stride: 2  # 2
  dropout: 0.1