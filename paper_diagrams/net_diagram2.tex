% A schematic diagram showing the shape of the feature map as it passes through the network.

% In the encoding path, the feature map is transformed with two successive 3x3x3 convolutions and ReLU activations
% followed by a 2x2x2 max-pooling operation that reduces the spatial dimensions by half.

% In the decoding path, information from fine and coarse scales is combined through skip connections.
% The feature map is upsampled by a factor of 2 using a 3x3x3 transposed convolution.
% This is combined with the feature map from the contracting path through a skip connection,
% which is passed through an attention gate before being concatenated with the upsampled feature map.
% The feature map from deeper in the network acts as the attention gating signal.
% The upsampled feature map and attended feature map from the skip connection are concatenated and
% convolved along the channel dimension to produce the final feature map.

\documentclass{standalone}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows.meta, positioning, calc}

% Styles - blocks
\tikzstyle{block} = [rectangle, draw, text centered, minimum width=2cm, minimum height=1cm, fill=white]
\tikzstyle{attention} = [circle, draw, text centered, minimum size=1cm, fill=orange!20]
\tikzstyle{concatmerge} = [circle, draw, text centred, minimum size=0.5cm]

% arrows
\tikzstyle{conv} = [thick, ->, >=stealth, black]
\tikzstyle{pool} = [thick, ->, >=stealth, green]
\tikzstyle{up}   = [thick, ->, >=stealth, orange]
\tikzstyle{skip} = [thick, dashed, ->, >=stealth, gray]
\tikzstyle{gate} = [thick, ->, >=stealth, blue]
\tikzstyle{merge} = [thick, ->, >=stealth, red]


\begin{document}
\begin{tikzpicture}[node distance=1.8cm and 0.5cm]

	% Input image and patch
	\node (input_full) [block, fill=gray!10] {Original Image $1\times192^3$};
	\node (input_patch) [block, below=1.5cm of input_full, fill=gray!30, align=center] {Extracted Patch\\$1\times160^3$};

	% Encoder
	\node (enc0a) [block, below=2.5cm of input_patch] {$8\times160^3$};
	\node (enc0b) [block, below left=2cm and 1cm of enc0a] {$16\times160^3$};

	\node (enc1a) [block, right=1cm of enc0b] {$16\times80^3$};
	\node (enc1b) [block, below left=2cm and 1cm of enc1a] {$32\times80^3$};

	\node (enc2a) [block, right=1cm of enc1b] {$32\times40^3$};
	\node (enc2b) [block, below left=2cm and 1cm of enc2a] {$64\times40^3$};

	\node (enc3a) [block, right=1cm of enc2b] {$64\times20^3$};
	\node (enc3b) [block, below left=2cm and 1cm of enc3a] {$128\times20^3$};

	\node (enc4a) [block, right=1cm of enc3b] {$128\times10^3$};
	\node (enc4b) [block, below left=1cm and 1cm of enc4a] {$256\times10^3$};

	% Bottleneck
	\node (bottom) [block, below right=1cm and 1cm of enc4b] {$256\times5^3$};

	% Attention Gates
	\node (att4) [attention, right=1cm of enc4a] {$\sigma$};
	\node (dec4up) [block, above right=1cm and 1cm of bottom]   {$128\times10^3$};
	\node (dec4concat) [block, right=1cm of att4] {$128\times10^3$};

	\node (att3) [attention, right=1cm of enc3a] {$\sigma$};
	\node (dec3up) [block, right=2cm of att3]   {$64\times20^3$};
	\node (dec3concat) [block, above=0.5cm of att3] {$64\times20^3$};

	\node (att2) [attention, right=1cm of enc2a] {$\sigma$};
	\node (dec2up) [block, right=2cm of att2]   {$32\times40^3$};
	\node (dec2concat) [block, above=0.5cm of att2] {$32\times40^3$};

	\node (att1) [attention, right=1cm of enc1a] {$\sigma$};
	\node (dec1up) [block, right=2cm of att1]   {$16\times80^3$};
	\node (dec1concat) [block, above=0.5cm of att1] {$16\times80^3$};

	\node (att0) [attention, right=1cm of enc0a] {$\sigma$};
	\node (dec0up) [block, right=2cm of att1] {$8\times160^3$};
	\node (dec0concat) [block, above=0.5cm of att0] {$8\times160^3$};

	\node (output_patch) [block, above=1cm of dec0concat, fill=green!20, align=center] {Output\\$1\times160^3$};
	\node (output_full) [block, fill=green!20, above=1.5cm of output_patch] {Segmentation Mask$1\times192^3$};

	% Arrows - conv
	\draw[conv] (enc0a) -- (enc0b);
	\draw[conv] (enc1a) -- (enc1b);
	\draw[conv] (enc2a) -- (enc2b);
	\draw[conv] (enc3a) -- (enc3b);
	\draw[conv] (enc4a) -- (enc4b);

	% Arrows - Encoder
	\draw[conv] (input_patch) -- (enc0a);
	\draw[pool] (enc0b) -- (enc1a);
	\draw[pool] (enc1b) -- (enc2a);
	\draw[pool] (enc2b) -- (enc3a);
	\draw[pool] (enc3b) -- (enc4a);
	\draw[pool] (enc4b) -- (bottom);

	% Skip Connections with Attention
	\draw[skip] (enc4a) -- (att4);
	\draw[skip] (enc3a) -- (att3);
	\draw[skip] (enc2a) -- (att2);
	\draw[skip] (enc1a) -- (att1);
	\draw[skip] (enc0a) -- (att0);

	% Gating Signals
	% \draw[gate] (dec4up) -- (att4);
	% \draw[gate] (dec3up) -- (att3);
	% \draw[gate] (dec2up) -- (att2);
	% \draw[gate] (dec1up) -- (att1);
	% \draw[gate] (dec0up) -- (att0);

	% Merge convolutions

	% Legend
	\matrix[draw, below left=-2cm and 2cm of input_patch, column sep=0.4cm, row sep=0.3cm, font=\large] {
		\draw[conv] (0,-0.3) -- +(0.6,0.0); & \node[align=left, text width=4cm] {(Conv3D + ReLU) $\times2$}; \\
		\draw[pool] (0,-0.3) -- +(0.6,0.0); & \node[align=left, text width=4cm] {Max Pooling}; \\
		\draw[up]   (0,-0.3) -- +(0.6,0.0); & \node[align=left, text width=4cm] {Transposed Conv}; \\
		\draw[skip] (0,-0.3) -- +(0.6,0.0); & \node[align=left, text width=4cm] {Skip Connection}; \\
		\draw[gate] (0,-0.3) -- +(0.6,0.0); & \node[align=left, text width=4cm] {Gating Signal}; \\
		\draw[merge] (0,-0.3) -- +(0.6,0.0); & \node[align=left, text width=4cm] {Convolution}; \\
	};

\end{tikzpicture}
\end{document}
