% A schematic diagram showing the shape of the feature map as it passes through the network.

% In the encoding path, the feature map is transformed with two successive 3x3x3 convolutions and ReLU activations
% followed by a 2x2x2 max-pooling operation that reduces the spatial dimensions by half.

% In the decoding path, information from fine and coarse scales is combined through skip connections.
% The feature map is upsampled by a factor of 2 using a 3x3x3 transposed convolution.
% This is combined with the feature map from the contracting path through a skip connection,
% which is passed through an attention gate before being concatenated with the upsampled feature map.
% The feature map from deeper in the network acts as the attention gating signal.
% The upsampled feature map and attended feature map from the skip connection are concatenated and
% convolved along the channel dimension to produce the final feature map.

\documentclass{standalone}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows.meta, positioning, calc}

% Styles - blocks
\tikzstyle{block} = [rectangle, draw, text centered, minimum width=2cm, minimum height=1cm, fill=white]
\tikzstyle{attention} = [circle, draw, text centered, minimum size=1cm, fill=orange!20]
\tikzstyle{concat} = [draw, dotted, minimum width=0.5cm, minimum height=0.5cm, text centered]

% arrows
\tikzstyle{conv} = [thick, ->, >=stealth, black]
\tikzstyle{pool} = [thick, ->, >=stealth, green]
\tikzstyle{up}   = [thick, ->, >=stealth, orange]
\tikzstyle{skip} = [thick, dashed, ->, >=stealth, gray]
\tikzstyle{gate} = [thick, ->, >=stealth, blue]
\tikzstyle{merge} = [thick, ->, >=stealth, red]


\begin{document}
\begin{tikzpicture}[node distance=1.8cm and 0.5cm]

	% Input image and patch
	\node (input_full) [block, fill=gray!10] {Original Image $1\times192^3$};
	\node (input_patch) [block, below=1.5cm of input_full, fill=gray!30, align=center] {Extracted Patch\\$1\times160^3$};

	% Encoder
	\node (enc0a) [block, below=2.5cm of input_patch] {$8\times160^3$};
	\node (enc0b) [block, below left=2cm and 1cm of enc0a] {$16\times160^3$};

	\node (enc1a) [block, right=1cm of enc0b] {$16\times80^3$};
	\node (enc1b) [block, below left=2cm and 1cm of enc1a] {$32\times80^3$};

	\node (enc2a) [block, right=1cm of enc1b] {$32\times40^3$};
	\node (enc2b) [block, below left=2cm and 1cm of enc2a] {$64\times40^3$};

	\node (enc3a) [block, right=1cm of enc2b] {$64\times20^3$};
	\node (enc3b) [block, below left=2cm and 1cm of enc3a] {$128\times20^3$};

	\node (enc4a) [block, right=1cm of enc3b] {$128\times10^3$};
	\node (enc4b) [block, below=1cm of enc4a] {$256\times10^3$};

	% Bottleneck
	\node (bottom) [block, right=1cm of enc4b] {$256\times5^3$};

	% Attention Gates
	\node (att4) [attention, above=1cm of bottom] {$\sigma$};
	\node (dec4skip) [concat, right=1.5cm of att4, minimum height=1cm, minimum width=2cm]   {$128\times10^3$};
	\node (dec4up) [block, below=0cm of dec4skip]   {$128\times10^3$};
	\node (dec4concat) [block, above=0.5cm of att4] {$128\times10^3$};

	\node (att3) [attention, above=0.5cm of dec4concat] {$\sigma$};
	\node (dec3skip) [concat, right=1.5cm of att3, minimum height=1cm, minimum width=2cm]   {$64\times20^3$};
	\node (dec3up) [block, below=0cm of dec3skip]   {$64\times20^3$};
	\node (dec3concat) [block, above=0.5cm of att3] {$64\times20^3$};

	\node (att2) [attention, above=0.5cm of dec3concat] {$\sigma$};
	\node (dec2skip) [concat, right=1.5cm of att2, minimum height=1cm, minimum width=2cm]   {$32\times40^3$};
	\node (dec2up) [block, below=0cm of dec2skip]   {$32\times40^3$};
	\node (dec2concat) [block, above=0.5cm of att2] {$32\times40^3$};

	\node (att1) [attention, above=0.5cm of dec2concat] {$\sigma$};
	\node (dec1skip) [concat, right=1.5cm of att1, minimum height=1cm, minimum width=2cm]   {$16\times80^3$};
	\node (dec1up) [block, below=0cm of dec1skip]   {$16\times80^3$};
	\node (dec1concat) [block, above=0.5cm of att1] {$16\times80^3$};

	\node (att0) [attention, above=0.5cm of dec1concat] {$\sigma$};
	\node (dec0skip) [concat, right=1.5cm of att0, minimum height=1cm, minimum width=2cm] {$8\times160^3$};
	\node (dec0up) [block, below=0cm of dec0skip] {$8\times160^3$};
	\node (dec0concat) [block, above=0.5cm of dec0skip] {$8\times160^3$};

	\node (output_patch) [block, above=1cm of dec0concat, fill=green!20, align=center] {Output\\$1\times160^3$};
	\node (output_full) [block, fill=green!20, above=1.5cm of output_patch] {Segmentation Mask$1\times192^3$};

	% Arrows - conv
	\draw[conv] (enc0a) -- (enc0b);
	\draw[conv] (enc1a) -- (enc1b);
	\draw[conv] (enc2a) -- (enc2b);
	\draw[conv] (enc3a) -- (enc3b);
	\draw[conv] (enc4a) -- (enc4b);

	% Arrows - Encoder
	\draw[conv] (input_patch) -- (enc0a);
	\draw[pool] (enc0b) -- (enc1a);
	\draw[pool] (enc1b) -- (enc2a);
	\draw[pool] (enc2b) -- (enc3a);
	\draw[pool] (enc3b) -- (enc4a);
	\draw[pool] (enc4b) -- (bottom);

	% Arrows - Decoder
	\draw[up] (bottom.north east) -- (dec4up);
	\draw[up] (dec4concat.north east) -- (dec3up);
	\draw[up] (dec3concat.north east) -- (dec2up);
	\draw[up] (dec2concat.north east) -- (dec1up);
	\draw[up] (dec1concat.north east) -- (dec0up);
	\draw[conv] (dec0concat) -- (output_patch);

	% Skip Connections with Attention
	\draw[skip] (enc4a) -- (att4);
	\draw[skip] (att4) -- (dec4skip);
	\draw[skip] (enc3a) -- (att3);
	\draw[skip] (att3) -- (dec3skip);
	\draw[skip] (enc2a) -- (att2);
	\draw[skip] (att2) -- (dec2skip);
	\draw[skip] (enc1a) -- (att1);
	\draw[skip] (att1) -- (dec1skip);
	\draw[skip] (enc0a) -- (att0);
	\draw[skip] (att0) -- (dec0skip);

	% Gating Signals
	\draw[gate] (bottom) -- (att4);
	\draw[gate] (dec4concat) -- (att3);
	\draw[gate] (dec3concat) -- (att2);
	\draw[gate] (dec2concat) -- (att1);
	\draw[gate] (dec1concat) -- (att0);

	% Merge convolutions
	\draw[merge] (dec4skip.north west) -- (dec4concat);
	\draw[merge] (dec3skip.north west) -- (dec3concat);
	\draw[merge] (dec2skip.north west) -- (dec2concat);
	\draw[merge] (dec1skip.north west) -- (dec1concat);
	\draw[merge] (dec0skip.north) -- (dec0concat);

	% Legend
	\matrix[draw, below left=-2cm and 2cm of input_patch, column sep=0.4cm, row sep=0.3cm, font=\large] {
		\draw[conv] (0,-0.3) -- +(0.6,0.0); & \node[align=left, text width=4cm] {(Conv3D + ReLU) $\times2$}; \\
		\draw[pool] (0,-0.3) -- +(0.6,0.0); & \node[align=left, text width=4cm] {Max Pooling}; \\
		\draw[up]   (0,-0.3) -- +(0.6,0.0); & \node[align=left, text width=4cm] {UpConv}; \\
		\draw[skip] (0,-0.3) -- +(0.6,0.0); & \node[align=left, text width=4cm] {Skip Connection}; \\
		\draw[gate] (0,-0.3) -- +(0.6,0.0); & \node[align=left, text width=4cm] {Gating Signal}; \\
		\draw[merge] (0,-0.3) -- +(0.6,0.0); & \node[align=left, text width=4cm] {Convolution}; \\
		\node[concat] {};               & \node[align=left, text width=4cm] {Concatenation}; \\
	};

\end{tikzpicture}
\end{document}
